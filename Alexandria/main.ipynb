{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "from scripts.style_generation import get_style_genre\n",
        "from scripts.first_n_words import get_first_n_words\n",
        "from scripts.llm import ask_LLM\n",
        "from scripts.kg_content import extract_kg_content\n",
        "from scripts.minhash_vector import create_minhash_vector\n",
        "from scripts.reconstruction_content import extract_reconstruction_content\n",
        "from scripts.evaluate import evaluate_peformance\n",
        "import scripts.prompts\n",
        "import scripts.api_key"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load the dataset from Hugging Face\n",
        "dataset = pd.read_csv(\"dataset/ML-Arxiv-Papers.csv\")\n",
        "rows, columns = dataset.shape\n",
        "# Extract the 'train' split\n",
        "#train_dataset = dataset[\"train\"]\n",
        "\n",
        "# Create lists for titles and abstracts\n",
        "# titles = [entry['title'] for entry in train_dataset]\n",
        "# abstracts = [entry['abstract'] for entry in train_dataset]\n",
        "\n",
        "# Create a list with concatenated title and abstract for each sample\n",
        "concatenated_texts = dataset['abstract'] #[f\"{title} {abstract}\" for title, abstract in zip(titles, abstracts)]\n",
        "\n",
        "API_KEY = scripts.api_key.API_KEY\n",
        "\n",
        "\n",
        "stop_len = 5000\n",
        "\n",
        "model_name = \"NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO\"\n",
        "system_prompt = \"You are a very smart very intelligence assistant who is very helpful.\"\n",
        "\n",
        "all_kg_results = []\n",
        "all_reconstruction_results = []\n",
        "input_string_so_far_list = []\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "117592"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "rows"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [],
      "source": [
        "for input_text in concatenated_texts[:2]:\n",
        "\n",
        "    try:\n",
        "\n",
        "        writing_style = get_style_genre(get_first_n_words(input_text, 1000))\n",
        "\n",
        "        # sentences= text_to_sentences(input_text)\n",
        "        # sentences =sentences_to_large_strings(sentences)\n",
        "        sentences = [input_text]\n",
        "        # print(sentences)\n",
        "        # continue\n",
        "        current_kg = []\n",
        "        current_kg.append(\"<style_analysis>\" + writing_style + \"</style_analysis>\")\n",
        "        print(\"<style_analysis>\" + writing_style + \"</style_analysis>\")\n",
        "        segment_nr = 1\n",
        "        reconstruction_so_far = \"\"\n",
        "        input_string_so_far = \"\"\n",
        "        for sentence in sentences:\n",
        "            input_string_so_far += sentence\n",
        "            if len(input_string_so_far) > stop_len:\n",
        "                break\n",
        "            print(\"INPUT:\", sentence)\n",
        "            # print(\"-----\")\n",
        "            # '''\n",
        "            # prompt=\"\"\"INPUT_TEXT:\n",
        "            # \"\"\"+sentence+\"\"\"\n",
        "            # INSTRUCTION:\n",
        "            # Paraphrase the given input text so that every statement is rephrased into sentences that contain only three to ten words each.\n",
        "            #   Use a simple structure and make sure to retain all information, names, numbers, and dates from the original text, without losing\n",
        "            #     any information. The output text should consist exclusively of factual, neutrally phrased sentences that are three to ten words\n",
        "            #       long. All information must be preserved, but without any artistic nuances. Direct speech in the source text should not be\n",
        "            #         replicated as such, but it should be laid out in short sentences who said or did what in which order, ensuring a neutral,\n",
        "            #           information-rich text.\"\"\"\n",
        "    \n",
        "            # reply = ask_LLM ('NousResearch/Nous-Hermes-2-Mixtral-8x7B-DPO',\n",
        "            #   \"You are a very smart very intelligence assistant who is very helpful.\",\n",
        "            #     input_text , API_KEY ,temperature=0.5,top_p=0.95,max_tokens=1000, frequency_penalty=1.1,presence_penalty=1.1)\n",
        "            # '''\n",
        "\n",
        "            # Determine the slice of the last 50 elements (if the list has more than 50 elements)\n",
        "            current_kg_context = current_kg[-50:] if len(current_kg) > 50 else current_kg\n",
        "\n",
        "            # Concatenate the elements into a single string\n",
        "            current_kg_context = ' '.join(current_kg_context)\n",
        "\n",
        "            print(\".....................KG_format_example_prompt start.......................\")\n",
        "            text = scripts.prompts.KG_format_example_prompt(current_kg_context, sentence)\n",
        "            print(text)\n",
        "            print(\".....................KG_format_example_prompt end.......................\")\n",
        "\n",
        "            for i in range(2):\n",
        "                knowledge_graph_segment = ask_LLM(model_name,\n",
        "                                                \"You are a very smart very intelligence assistant who is very helpful.\",\n",
        "                                                text, API_KEY, temperature=0.5, top_p=0.95, max_tokens=1000,\n",
        "                                                frequency_penalty=1.1, presence_penalty=1.1)\n",
        "                if not (extract_kg_content(knowledge_graph_segment) == None):\n",
        "                    break\n",
        "            try:\n",
        "                current_kg.append(\"<segment \" + str(segment_nr) + \">\\n\" + extract_kg_content(\n",
        "                    knowledge_graph_segment) + \"<source_sentence_min_hash: \" + str(\n",
        "                    create_minhash_vector(sentence)) + \" >\\n\" + \"</segment \" + str(segment_nr) + \">\\n\")\n",
        "                print(\"<segment \" + str(segment_nr) + \">\\n\" + extract_kg_content(\n",
        "                    knowledge_graph_segment) + \"<source_sentence_min_hash: \" + str(\n",
        "                    create_minhash_vector(sentence)) + \" >\\n\" + \"</segment \" + str(segment_nr) + \">\\n\")\n",
        "            except:\n",
        "                current_kg.append(\n",
        "                    \"<segment \" + str(segment_nr) + \">\\n\" + knowledge_graph_segment + \"<source_sentence_min_hash: \" + str(\n",
        "                        create_minhash_vector(sentence)) + \" >\\n\" + \"</segment \" + str(segment_nr) + \">\\n\")\n",
        "                print(\"<segment \" + str(segment_nr) + \">\\n\" + knowledge_graph_segment + \"<source_sentence_min_hash: \" + str(\n",
        "                    create_minhash_vector(sentence)) + \" >\\n\" + \"</segment \" + str(segment_nr) + \">\\n\")\n",
        "\n",
        "            prompt = scripts.prompts.KG_reconstruction_prompt(reconstruction_so_far, current_kg)\n",
        "            for i in range(2):\n",
        "                next_reconstruction = ask_LLM(model_name,\n",
        "                                            \"You are a very smart very intelligence assistant who is very helpful.\",\n",
        "                                            prompt, API_KEY, temperature=0.5, top_p=0.95, max_tokens=4000,\n",
        "                                            frequency_penalty=1.1, presence_penalty=1.1)\n",
        "                if not (extract_reconstruction_content(next_reconstruction) == None):\n",
        "                    break\n",
        "\n",
        "            reconstruction_so_far += extract_reconstruction_content(next_reconstruction)\n",
        "            print(extract_reconstruction_content(next_reconstruction))\n",
        "            segment_nr += 1\n",
        "        all_kg_results.append(current_kg)\n",
        "        all_reconstruction_results.append(reconstruction_so_far)\n",
        "        input_string_so_far_list.append(input_string_so_far)\n",
        "    except:\n",
        "        pass\n",
        "        "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "500 word sample evalution: \n",
            "\n",
            "No context correct answer percentage: 0 \n",
            "\n",
            "Original context correct answer percentage: 0 \n",
            "\n",
            "Knowledgegraph context correct answer percentage: 0 \n",
            "\n",
            "Reconstruckted text context correct answer percentage: 0 \n",
            "\n"
          ]
        }
      ],
      "source": [
        "df = pd.DataFrame({\n",
        "    'Input_Texts': input_string_so_far_list,\n",
        "    'Output_Graphs': all_kg_results,\n",
        "    'Output_Reconstructions': all_reconstruction_results, })\n",
        "\n",
        "\n",
        "# print(df)\n",
        "\n",
        "print(\"500 word sample evalution:\", \"\\n\")\n",
        "base_cap_500, original_cap_500, knowledgegraph_cap_500, reconstruction_cap_500,QA_df = evaluate_peformance(df, 2,\n",
        "                                                                                                     \"q_a_kg.parquet\")\n",
        "\n",
        "print(\"No context correct answer percentage:\", base_cap_500, \"\\n\")\n",
        "print(\"Original context correct answer percentage:\", original_cap_500, \"\\n\")\n",
        "print(\"Knowledgegraph context correct answer percentage:\", knowledgegraph_cap_500, \"\\n\")\n",
        "print(\"Reconstruckted text context correct answer percentage:\", reconstruction_cap_500, \"\\n\")\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {},
      "outputs": [],
      "source": [
        "# df.to_csv(\"dataset/df_save.csv\", encoding='utf-8', index=False)\n",
        "# QA_df.to_csv(\"dataset/questions_answer_save.csv\", encoding='utf-8', index=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#1000 paper for cot: \n",
        "\n",
        "# No context correct answer percentage: 44.19784400760939 \n",
        "\n",
        "# Original context correct answer percentage: 84.14647730437204 \n",
        "\n",
        "# Knowledgegraph context correct answer percentage: 77.47158824081902 \n",
        "\n",
        "# Reconstruckted text context correct answer percentage: 77.27733804656881 \n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python (notebook_env)",
      "language": "python",
      "name": "notebook_env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
